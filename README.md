# VLBiasBench
VLBiasBenchï¼š A large-scale dataset composed of high-quality synthetic images aimed at evaluating social biases in LVLMs{ font-family: Arial, sans-serif; margin: 0; padding: 0; display: flex; flex-direction: column; align-items: center; line-height: 1.6; /\* æ”¹å–„æ–‡æœ¬çš„å¯è¯»æ€§ \*/ } /\* å…¶ä»–æ ·å¼ä¸å˜ï¼Œè¿™é‡Œåªå±•ç¤ºä¿®æ”¹éƒ¨åˆ† \*/ .image-container { text-align: center; /\* ç¡®ä¿å›¾ç‰‡å’Œå›¾ç‰‡æ ‡é¢˜å±…ä¸­ \*/ } /\* åˆå¹¶.image-container imgçš„æ ·å¼ \*/ /\* ä¿æŒå…¶ä»–å›¾ç‰‡çš„æ ·å¼ä¸å˜ \*/ /\* ä¿æŒå…¶ä»–å›¾ç‰‡çš„æ ·å¼ä¸å˜ \*/ /\*.image-container img {\*/ /\* width:95%;\*/ /\* display: block; !\* imgé»˜è®¤æ˜¯inlineå…ƒç´ ï¼Œè¿™é‡Œæ”¹ä¸ºblockä»¥ä¾¿ä½¿ç”¨margin autoå±…ä¸­ \*!\*/ /\* margin-left: auto;\*/ /\* margin-right: auto;\*/ /\*}\*/ /\*!\* ç§»é™¤äº†é’ˆå¯¹ .overview-img å’Œ .subtasks-img è®¾ç½®å®½åº¦çš„è§„åˆ™ \*!\*/ /\*!\* ç‰¹åˆ«ä¸ºç¬¬äºŒå¼ å›¾è®¾ç½®ä¸€ä¸ªæ›´å°çš„å®½åº¦ï¼Œç¡®ä¿è¿™ä¸ªè§„åˆ™åœ¨æ ·å¼è¡¨ä¸­çš„ä½ç½® \*!\*/ /\*.subtasks-img {\*/ /\* width: 40%; !\* è®¾ç½®ç¬¬äºŒå¼ å›¾å®½åº¦ä¸º40% \*!\*/ /\*}\*/ /\* ç¡®ä¿.containerå†…çš„æ‰€æœ‰å…ƒç´ éƒ½èƒ½å¾—åˆ°é€‚å½“çš„å±…ä¸­å¤„ç† \*/ .container { text-align: left; /\* è¿™å°†å½±å“åˆ°å®¹å™¨å†…çš„è¡Œå†…å…ƒç´ å’Œæ–‡æœ¬å†…å®¹ \*/ margin-top: 20px; width: 80%; margin-left: auto; margin-right: auto; /\* æ°´å¹³å±…ä¸­ \*/ } .center { text-align: center; } .author-info sup { font-size: smaller; } .author-info, .institution, .email-contact { display: flex; justify-content: center; } .email-contact { margin-bottom: 20px; } /\* æ–°å¢æˆ–ä¿®æ”¹çš„å›¾ç‰‡æ ·å¼ \*/ .overview-img { width: 95%; height: auto; display: block; margin-left: auto; margin-right: auto; } .subtasks-img { width: 40%; height: auto; display: block; margin-left: auto; margin-right: auto; } /\*.overview-img, .subtasks-img {\*/ /\* width: 96%;\*/ /\* height: auto;\*/ /\* display: block;\*/ /\* margin-left: auto;\*/ /\* margin-right: auto;\*/ /\*}\*/ table { /\*è¡¨æ ¼ä½äºé¡µé¢å±…ä¸­ä½ç½®\*/ margin-left: auto; margin-right: auto; width: 50%; border-collapse: collapse; box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); /\* æ·»åŠ é˜´å½±æ•ˆæœ \*/ } table, th, td { border: 1px solid black; } th, td { padding: 8px; text-align: left; } th { background-color: #f2f2f2; } tr:nth-child(even) { background-color: #f9f9f9; /\* ä¸ºå¶æ•°è¡Œæ·»åŠ æ¡çº¹èƒŒæ™¯ \*/ } a { color: #007bff; /\* è®¾ç½®é“¾æ¥é¢œè‰² \*/ text-decoration: none; /\* ç§»é™¤ä¸‹åˆ’çº¿ \*/ } a:hover { text-decoration: underline; /\* é¼ æ ‡æ‚¬åœæ—¶æ·»åŠ ä¸‹åˆ’çº¿ \*/ } @media (max-width: 768px) { .container, .image-container img { width: 95%; /\* åœ¨å°å±è®¾å¤‡ä¸Šè°ƒæ•´å®¹å™¨å’Œå›¾ç‰‡å®½åº¦ \*/ } }

![](figure/icon0.webp)

## ğŸ¨Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception Ability of LVLMs

ğŸ”—The link to our project is [\[here\]](https://github.com/Robin-WZQ/LADS)

### OverviewğŸ”

![Overview of the automatic pipeline in Dysca](./figure/dysca_framework.svg)

**Figure 1. Overview of the automatic pipeline in Dysca for generating VQAs, cleaning VQAs and evaluating LVLMs.**

  

![The available subtasks of our Dysca](./figure/subtasks.svg)

**Figure 2. The available subtasks of our Dysca.**

**Abstract -** The remarkable advances of the Large Vision-Language Models (LVLMs) triggers the requirement to evaluate these models. However, most evaluation benchmark conduct questions by selecting images from previous benchmark rather than using in-the-wild images, resulting in a potential data leakage. Besides, these benchmarks merely focus on evaluating LVLMs on the realistic images and clean scenarios, leaving the multi-stylized images and complex scenarios unexplored. In response to these challenges, we propose Dysca, a dynamic and scalable benchmark for evaluating LVLMs by leveraging synthesis images. Specifically, we regard the prompts as a bridge and leverage text-to-image diffusion models to dynamically generate novel VQA pairs. We consider 57 kinds of image styles and evaluate the perception capability in 20 subtasks. Moreover, we conduct evaluations under 4 scenarios (i.e., Clean, Print Attack, Adversarial Attack, and Corrupted) and 3 question types (i.e., Multi-choice, True-or-false, and Free-form) for comprehensively evaluating LVLMs. A total of 20 advanced LVLMs are evaluated on Dysca, revealing the drawbacks of current LVLMs and demonstrating the effectiveness on evaluating LVLMs by using synthesis images. Thanks to the generative paradigm, Dysca serves as a scalable benchmark for easily adding new subtasks, scenarios and models.

### Key statistics of DyscağŸ“Š

| Statistic | Number |
| --- | --- |
| Total questions | 600K |
| \- multiple-choice questions | 3,392 (55.2%) |
| \- Free-form questions | 2,749 (44.8%) |
| \- True-or-false | 5,261 (85.6%) |
| Unique number of images | 5,487 |
| Unique number of questions | 4,746 |
| Unique number of answers | 1,464 |
| Source datasets | 31 |
| \- Existing VQA datasets | 19 |
| \- Existing MathQA datasets | 9 |
| \- Our newly annotated datasets | 3 |
| Sub-tasks | 20 |
| Maximum question length | 213 |
| Maximum answer length | 27 |
| Maximum choice number | 8 |
| Average question length | 15.6 |
| Average answer length | 1.2 |
| Average choice number | 3.4 |

### Examples of DyscağŸ“¸

### Evaluation ResultsğŸ†

#### Clean Scenario

| Rank | Model | Score |
| --- | --- | --- |
| ğŸ¥‡ | A |  |
| ğŸ¥ˆ | B |  |
| ğŸ¥‰ | C |  |
| 4 | D |  |
| 5 | E |  |

### GuidelinesğŸ§­

#### The folder "category"

The category folder contains all of Dysca's source material. It contains the following:

                `category â”œâ”€â”€ People â”‚   â”œâ”€â”€ Age.txt â”‚   â”œâ”€â”€ Emotion.txt â”‚   â”œâ”€â”€ Gender.txt â”‚   â”œâ”€â”€ Race.txt â”œâ”€â”€ Actions.txt â”œâ”€â”€ Profession.txt â”œâ”€â”€ Celebrity.txt â”œâ”€â”€ Animal.txt â”œâ”€â”€ Plant.txt â”œâ”€â”€ Food.txt â”œâ”€â”€ Object.txt â”œâ”€â”€ Landmarks.txt â”œâ”€â”€ Clothes.txt â”œâ”€â”€ Movie.txt â”œâ”€â”€ TV shows.txt â”œâ”€â”€ Anime.txt â”œâ”€â”€ Color.txt â”œâ”€â”€ Background.txt â”œâ”€â”€ Styles.json â”œâ”€â”€ ocr_text.json â”œâ”€â”€ text.txt`
                    
    

#### Generating Prompts, Questions and Answers

The `./code/prompt_question_answer.py` is used to generate the source data for the dataset, i.e., all the prompts, questions, and answers used to generate the images. Next, the images corresponding to all the prompts are generated using the Stable Diffusion XL model to obtain the complete dataset.

Specifically, in the main function's parameters:

+   "tasks" parameter can choose one from "recognition", "OCR".
+   "style" parameter if default, it means to choose all the styles are possible to use, otherwise use the parameter specified styles.
+   "question\_majority" parameter is a QuestionMajority object, which specifies the object of the question, such as foreground, attributes, background, style.
+   "question\_types" specifies which question types are selected.
+   "prompt\_num" specifies the upper limit of generated prompts.
+   "save\_dir" refers to the folder where the results are saved.

### Related projectsğŸ”—

+   [BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
+   [EMU2](https://github.com/baaivision/Emu)
+   [InstructBLIP](https://github.com/salesforce/LAVIS/blob/main/projects/instructblip)
+   [LLaVA-1.5](https://github.com/haotian-liu/LLaVA)
+   [miniGPT4](https://github.com/Vision-CAIR/MiniGPT-4)
+   [miniGPT-v2](https://github.com/Vision-CAIR/MiniGPT-4)
+   [Otter](https://github.com/Vision-CAIR/MiniGPT-4)
+   [Qwen-VL](https://github.com/QwenLM/Qwen-VL)
+   [Shikra](https://github.com/shikras/shikra)
+   [InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer)

### Cite our workğŸ“

        `coming soon...`
        
    

### Acknowledgement

This section will include acknowledgements...
